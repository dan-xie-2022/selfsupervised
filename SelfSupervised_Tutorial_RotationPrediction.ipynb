{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QhPdJIi2Nbd2"
      },
      "source": [
        "## Rotation Prediction\n",
        "In [1], Instead than predicting the relative position between patches of an image, authors propose to predict the geometric transformation applied to the entire image\n",
        "\n",
        "![](https://drive.google.com/uc?id=1eHXLH-N_6uMGRzdf1Wjnga26qlS5-FRv)\n",
        "\n",
        "They propose to use rotations by 0, 90, 180, 270 degrees as trasformations.\n",
        "\n",
        "[1] S. Gidaris et al. “Unsupervised Representation Learning by Predicting Image Rotations”. In: ICLR. 2018."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's load all libraries, we will use pytorch and pytorch vision, and check whether we are using CPU or GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "8LnJlAFB8jfv",
        "outputId": "43a96557-8b60-4615-db9b-91177ba03f7f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "torch.manual_seed(42) # Setting the seed\n",
        "\n",
        "## Torchvision\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import STL10\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "## Plot Options\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib_inline.backend_inline\n",
        "import seaborn as sns\n",
        "plt.set_cmap(\"cividis\")\n",
        "%matplotlib inline\n",
        "sns.set()\n",
        "\n",
        "\n",
        "# In this notebook, we use data loaders with heavier computational processing. It is recommended to use as many\n",
        "# workers as possible in a data loader, which corresponds to the number of CPU cores\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.determinstic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Ensure that you are using GPU and all CPU workers\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)\n",
        "print(\"Number of workers:\", NUM_WORKERS)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ABAd7xO_qA41"
      },
      "source": [
        "As Dataset, we will use the STL-10 dataset (https://cs.stanford.edu/~acoates/stl10/).\n",
        "It is an image recognition dataset for developing self-supervised and unsupervised feature learning, deep learning algorithms. It is inspired by the CIFAR-10 dataset but with some modifications. In particular, each class has fewer labeled training examples than in CIFAR-10, but a very large set of unlabeled examples is provided. The primary challenge is to make use of the unlabeled data (which comes from a similar but different distribution from the labeled data). W\n",
        "\n",
        "Overview of the dataset:\n",
        "- 10 classes: airplane, bird, car, cat, deer, dog, horse, monkey, ship, truck.\n",
        "- Images are 96x96 pixels, color.\n",
        "- 5000 training images (10 pre-defined folds), 8000 test images.\n",
        "- 100000 unlabeled images. These examples are extracted from a similar but broader distribution of images. For instance, it contains other types of animals (bears, rabbits, etc.) and vehicles (trains, buses, etc.) in addition to the ones in the labeled set.\n",
        "- Images were acquired from labeled examples on $ImageNet$. \n",
        "\n",
        "Pytorch proposes two practical data primitives, called DataSet and DataLoader, to handle datasets for pre-processing and training.\n",
        "There are many data-sets pre-loaded, such as STL-10. For more info: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "\n",
        "In particular, we can easily compose several data-augmentations (transformations), that are automatically applied to the images of the dataset. Here we have an exemple with the transformations used in [1]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXhGEmnT9voP",
        "outputId": "f7b66dad-9b21-43e5-95e7-1626fea3a288"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import STL10\n",
        "\n",
        "transformTrain = transforms.Compose([\n",
        "         transforms.RandomResizedCrop(224),\n",
        "         transforms.RandomHorizontalFlip(),\n",
        "         transforms.ToTensor(),\n",
        "         transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "         ])\n",
        "\n",
        "transformTest = transforms.Compose([\n",
        "          transforms.Resize(256),\n",
        "          transforms.CenterCrop(224),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "         ])\n",
        "\n",
        "# We will only \n",
        "#unlabeled_dataset = STL10(root=\"./data\", split=\"unlabeled\", download=True, transform=transformTrain)\n",
        "train_dataset = STL10(root=\"./data\", split=\"train\", download=True, transform=transformTest)\n",
        "test_dataset = STL10(root=\"./data\", split=\"test\", download=True, transform=transformTest)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Kioo1xFkQVRa"
      },
      "source": [
        "The Unlabeled dataset contains 100k images.\n",
        "\n",
        "Here, we will use the Train dataset (as if it was unlabeled) since it contains 5k images.\n",
        "\n",
        "If you want more images, you can use 'unlabeled_dataset_red' which contains 10% of the unlabeled dataset (i.e., 10K images), and if you want less images (i.e., 1k), you can add 'folds=0' to the train_dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMXiU1qX2dqu",
        "outputId": "eda05afc-fb50-40b8-efac-098e37fbc797"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "# To check the classes in STL10\n",
        "#classes=unlabeled_dataset.classes\n",
        "#print(classes)\n",
        "#print('Number images in Unlabeled dataset:' ,len(unlabeled_dataset))\n",
        "\n",
        "# We reduce number unlabeled images for computational reasons\n",
        "#sizeUnlabelled=5000\n",
        "\n",
        "#unlabeled_dataset_red,rest = random_split(unlabeled_dataset, [sizeUnlabelled, len(unlabeled_dataset)-sizeUnlabelled])\n",
        "#len(unlabeled_dataset_red)\n",
        "#del unlabeled_dataset # free memory\n",
        "\n",
        "\n",
        "# Train dataset\n",
        "labels=train_dataset.labels # retrieve label of each sample\n",
        "#print(labels)\n",
        "print('Number images in Train dataset:' , len(train_dataset)) # retrieve length of dataset\n",
        "print(train_dataset[3][0].shape) # this is one image\n",
        "print(train_dataset[3][1]) # this is a label\n",
        "\n",
        "#Test dataset\n",
        "print('Number images in Test dataset:' ,len(test_dataset))\n",
        "print(test_dataset[0][0].shape) # this is one image"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's plot few images to see how are they. We need to de-normalize images using the statics of ImageNet (images come from ImageNet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIH9otn52ZR1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "def imshowSTL10(dataset,rows=3,columns=3,figsize=(8, 8)):\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    for i in range(1, rows*columns+1):\n",
        "      img = dataset[i][0]\n",
        "\n",
        "      #REMOVE NORMALIZATION\n",
        "      mean = torch.tensor([0.485, 0.456, 0.406])\n",
        "      std = torch.tensor([0.229, 0.224, 0.225])\n",
        "      unnormalize = transforms.Normalize((-mean / std).tolist(), (1.0 / std).tolist())\n",
        "      # Clip values to range [0,1] -> possible rounding errors during normalization\n",
        "      img = np.clip(unnormalize(img).numpy(),0,1)\n",
        "\n",
        "      label = dataset[i][1]\n",
        "      fig.add_subplot(rows, columns, i)\n",
        "      plt.title(dataset.classes[label])\n",
        "      plt.imshow(np.transpose(img, (1, 2, 0)))\n",
        "      plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "8KmQg476Jqtl",
        "outputId": "6bb21d4c-98b8-4fb2-ffdc-af296b1489e9"
      },
      "outputs": [],
      "source": [
        "imshowSTL10(train_dataset)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1fSH4ONZkSp"
      },
      "source": [
        "Now it's time to implement our network.\n",
        "\n",
        "Since STL10 comes from ImageNet, we take the same network proposed for ImageNet in [1], namely AlexNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5toYrq0ZOFJ"
      },
      "outputs": [],
      "source": [
        "# Code borrowed from https://github.com/gidariss/FeatureLearningRotNet\n",
        "# AlexNet\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AlexNet, self).__init__()\n",
        "        num_classes = 4\n",
        "\n",
        "        conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        conv2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(192),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        conv3 = nn.Sequential(\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        conv4 = nn.Sequential(\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        conv5 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        pool5 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "\n",
        "        num_pool5_feats = 6 * 6 * 256\n",
        "        fc_block = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(num_pool5_feats, 4096, bias=False),\n",
        "            nn.BatchNorm1d(4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, 4096, bias=False),\n",
        "            nn.BatchNorm1d(4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        classifier = nn.Sequential(\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "        self._feature_blocks = nn.ModuleList([\n",
        "            conv1,\n",
        "            pool1,\n",
        "            conv2,\n",
        "            pool2,\n",
        "            conv3,\n",
        "            conv4,\n",
        "            conv5,\n",
        "            pool5,\n",
        "            fc_block,\n",
        "            classifier,\n",
        "        ])\n",
        "        self.all_feat_names = [\n",
        "            'conv1',\n",
        "            'pool1',\n",
        "            'conv2',\n",
        "            'pool2',\n",
        "            'conv3',\n",
        "            'conv4',\n",
        "            'conv5',\n",
        "            'pool5',\n",
        "            'fc_block',\n",
        "            'classifier',\n",
        "        ]\n",
        "        assert(len(self.all_feat_names) == len(self._feature_blocks))\n",
        "\n",
        "    def _parse_out_keys_arg(self, out_feat_keys):\n",
        "\n",
        "        # By default return the features of the last layer / module.\n",
        "        out_feat_keys = [self.all_feat_names[-1],] if out_feat_keys is None else out_feat_keys\n",
        "\n",
        "        if len(out_feat_keys) == 0:\n",
        "            raise ValueError('Empty list of output feature keys.')\n",
        "        for f, key in enumerate(out_feat_keys):\n",
        "            if key not in self.all_feat_names:\n",
        "                raise ValueError('Feature with name {0} does not exist. Existing features: {1}.'.format(key, self.all_feat_names))\n",
        "            elif key in out_feat_keys[:f]:\n",
        "                raise ValueError('Duplicate output feature key: {0}.'.format(key))\n",
        "\n",
        "        # Find the highest output feature in `out_feat_keys\n",
        "        max_out_feat = max([self.all_feat_names.index(key) for key in out_feat_keys])\n",
        "\n",
        "        return out_feat_keys, max_out_feat\n",
        "\n",
        "    def forward(self, x, out_feat_keys=None):\n",
        "        \"\"\"Forward an image `x` through the network and return the asked output features.\n",
        "        Args:\n",
        "          x: input image.\n",
        "          out_feat_keys: a list/tuple with the feature names of the features\n",
        "                that the function should return. By default the last feature of\n",
        "                the network is returned.\n",
        "        Return:\n",
        "            out_feats: If multiple output features were asked then `out_feats`\n",
        "                is a list with the asked output features placed in the same\n",
        "                order as in `out_feat_keys`. If a single output feature was\n",
        "                asked then `out_feats` is that output feature (and not a list).\n",
        "        \"\"\"\n",
        "        out_feat_keys, max_out_feat = self._parse_out_keys_arg(out_feat_keys)\n",
        "        out_feats = [None] * len(out_feat_keys)\n",
        "\n",
        "        feat = x\n",
        "        for f in range(max_out_feat+1):\n",
        "            feat = self._feature_blocks[f](feat)\n",
        "            key = self.all_feat_names[f]\n",
        "            if key in out_feat_keys:\n",
        "                out_feats[out_feat_keys.index(key)] = feat\n",
        "\n",
        "        out_feats = out_feats[0] if len(out_feats)==1 else out_feats\n",
        "        return out_feats\n",
        "\n",
        "    def get_L1filters(self):\n",
        "        convlayer = self._feature_blocks[0][0]\n",
        "        batchnorm = self._feature_blocks[0][1]\n",
        "        filters = convlayer.weight.data\n",
        "        scalars = (batchnorm.weight.data / torch.sqrt(batchnorm.running_var + 1e-05))\n",
        "        filters = (filters * scalars.view(-1, 1, 1, 1).expand_as(filters)).cpu().clone()\n",
        "\n",
        "        return filters"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And here it's the most important part of the code. \n",
        "\n",
        "We build the RotationPrediction Module.\n",
        "\n",
        "As in [1], we use the 'conv5' layer as representation features and a small classifier, as proposed in the paper.\n",
        "\n",
        "The key part is in the 'forward' function and in 'preprocess'. Can you see what does the algorithm do ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glv383BjZuYl"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RotationPrediction(nn.Module):\n",
        "    metrics = ['Loss', 'Acc1']\n",
        "    metrics_fmt = [':.4e', ':6.2f']\n",
        "\n",
        "    def __init__(self, dataset, n_classes, device):\n",
        "        super().__init__()\n",
        "        self.device=device\n",
        "        self.model = AlexNet().to(self.device)\n",
        "        self.latent_dim = 256 * 13 * 13\n",
        "        self.feat_layer = 'conv5'\n",
        "        self.dataset = dataset\n",
        "        self.n_classes = n_classes\n",
        "        \n",
        "\n",
        "    def construct_classifier(self):\n",
        "        classifier = nn.Sequential(\n",
        "            nn.AdaptiveMaxPool2d((6, 6)),\n",
        "            nn.BatchNorm2d(256, affine=False),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 6 * 6, self.n_classes)\n",
        "        )\n",
        "        classifier=classifier.to(self.device)\n",
        "        return classifier\n",
        "\n",
        "    def forward(self, images):\n",
        "        images=images.to(self.device)\n",
        "        batch_size = images.shape[0]\n",
        "        images, targets = self._preprocess(images)\n",
        "        targets = targets.to(self.device)\n",
        "\n",
        "        logits, zs = self.model(images, out_feat_keys=('classifier', self.feat_layer))\n",
        "        loss = F.cross_entropy(logits, targets).to(self.device)\n",
        "\n",
        "        pred = logits.argmax(dim=-1)\n",
        "        correct = pred.eq(targets).float().sum()\n",
        "        acc = correct / targets.shape[0] * 100.\n",
        "\n",
        "        zs = zs[:batch_size]\n",
        "        return dict(Loss=loss, Acc1=acc), zs[:batch_size]\n",
        "\n",
        "    def encode(self, images, flatten=True):\n",
        "        zs = self.model(images, out_feat_keys=(self.feat_layer,))\n",
        "        return zs.flatten(start_dim=1)\n",
        "\n",
        "    def _preprocess(self, images):\n",
        "        batch_size = images.shape[0]\n",
        "        # We just use the transpose to rotate images. Important\n",
        "        images_90 = torch.flip(images.transpose(2, 3), (2,))\n",
        "        images_180 = torch.flip(images, (2, 3))\n",
        "        images_270 = torch.flip(images, (2,)).transpose(2, 3)\n",
        "        # These are the Images to Predict\n",
        "        images_batch = torch.cat((images, images_90, images_180, images_270), dim=0)\n",
        "        # Target are Labels (0,1,2,3)\n",
        "        targets = torch.arange(4).long().repeat(batch_size)\n",
        "        targets = targets.view(batch_size, 4).transpose(0, 1)\n",
        "        targets = targets.contiguous().view(-1)\n",
        "        return images_batch, targets\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we train our algorithm and evaluate its prediction power on a test set (function 'validate') using the (classical) accuracy function.\n",
        "\n",
        "Do you understand what the function accuracy does ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxeFKc9ObBT_"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# output: (batch_size, n_logits)\n",
        "# target (batch_size, 1)\n",
        "# target contains ground truth, the integer class label.\n",
        "# top-k accuracy: counting how many ground truth labels are among the k highest predictions (logits) of the output.  \n",
        "# \n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for\n",
        "    the specified values of k.\n",
        "\n",
        "    Args:\n",
        "        output (torch.Tensor): prediction matrix with shape (batch_size, num_classes).\n",
        "        target (torch.LongTensor): ground truth labels with shape (batch_size).\n",
        "        topk (tuple, optional): accuracy at top-k will be computed. For example,\n",
        "            topk=(1, 5) means accuracy at top-1 and top-5 will be computed. \n",
        "            It will count how many ground truth labels are among the k highest \n",
        "            predictions (logits) of the output, namely where the network is more sure. \n",
        "\n",
        "    Returns:\n",
        "        list: accuracy at top-k.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "        \n",
        "        _, pred = output.topk(k=maxk, dim=1, largest=True, sorted=True) # sliced torch.argsort (for each row, it outputs the indices containing the k max values)\n",
        "        pred = pred.t() # transpose to get batches as last dimension\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred)) # first transpose then expand target to be the same size as pred and then elemntwise-equality\n",
        "\n",
        "        # compute number of samples in the batch that has correct label in the k highest predictions\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)  \n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "def train(train_loader, model, linear_classifier, optimizer, optimizer_linear, epoch, device):\n",
        "\n",
        "    model.train()\n",
        "    linear_classifier.train()\n",
        "    top1=[]\n",
        "    top5=[]\n",
        "\n",
        "    for i, (images, target) in enumerate(train_loader):\n",
        "        # compute loss\n",
        "        bs = images.shape[0]\n",
        "        images = images.to(device)\n",
        "        target = target.to(device)\n",
        "        out, zs = model(images)\n",
        "        zs = zs.detach() # detach from the graph,  requires_grad = False\n",
        "\n",
        "        # compute gradient and optimizer step for ssl task\n",
        "        optimizer.zero_grad()\n",
        "        out['Loss'].backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # compute loss\n",
        "        logits = linear_classifier(zs)\n",
        "        loss = F.cross_entropy(logits, target)\n",
        "\n",
        "        acc1, acc5 = accuracy(logits, target, topk=(1, 5))\n",
        "        top1.append(acc1[0])\n",
        "        top5.append(acc5[0])\n",
        "\n",
        "        optimizer_linear.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer_linear.step()\n",
        "\n",
        "        if i % 5 == 0:\n",
        "            print('Epoch {}, Iter: {} out of {}, Average loss: {:.4f}, acc 1: {:.4f}'.format(epoch, i, len(train_loader), loss / len(train_loader.dataset),acc1[0]))\n",
        "\n",
        "    print('Epoch: {}, Average loss: {:.4f}, Average acc 1: {:.4f}, Average acc 5: {:.4f}'.format(epoch, loss / len(train_loader.dataset),sum(top1)/len(top1), sum(top5)/len(top5)))    \n",
        "    return sum(top1)/len(top1), sum(top5)/len(top5)\n",
        "\n",
        "def validate(val_loader, model, linear_classifier, device):\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "    linear_classifier.eval()\n",
        "    top1=[]\n",
        "    top5=[]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (images, target) in enumerate(val_loader):\n",
        "            # compute and measure loss\n",
        "            bs = images.shape[0]\n",
        "            images = images.to(device)\n",
        "            target = target.to(device)\n",
        "            out, zs = model(images)\n",
        "\n",
        "            # compute loss\n",
        "            logits = linear_classifier(zs)\n",
        "            loss = F.cross_entropy(logits, target)\n",
        "            acc1, acc5 = accuracy(logits, target, topk=(1, 5))\n",
        "            top1.append(acc1[0])\n",
        "            top5.append(acc5[0])\n",
        "\n",
        "    print('Validation: Average loss: {:.4f}, Average acc 1: {:.4f}, Average acc 5: {:.4f}'.format(loss / len(train_loader.dataset), sum(top1)/len(top1), sum(top5)/len(top5)))\n",
        "    return sum(top1)/len(top1), sum(top5)/len(top5)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we define the:\n",
        "- DataLoader, a pytorch object that wraps an iterable around the Dataset to enable easy access to the samples duing training or validation\n",
        "- optimization process\n",
        "- the sceduler \n",
        "- all hyper-parameters (max epochs, batch size, lr, weight decay,etc.)\n",
        "\n",
        "Please note that it will lauch the training process. It can last some time... OR\n",
        "\n",
        "you can directly load a pre-trained model in the following cell !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "8OJUWIj6byHl",
        "outputId": "b3208f14-2034-4f26-ee86-5378c2f05080"
      },
      "outputs": [],
      "source": [
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "maxepochs=150 # maximum number of epochs\n",
        "bs=128 # batch size\n",
        "lr_initial=0.01 # initial learning rate\n",
        "wd=5e-4 # weight decay\n",
        "# Ensure that you are using GPU and all CPU workers\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "\n",
        "#unlabeled_loader = data.DataLoader(unlabeled_dataset_red, batch_size=bs, num_workers=NUM_WORKERS,shuffle=True)\n",
        "train_loader = data.DataLoader(train_dataset, batch_size=bs, num_workers=NUM_WORKERS, shuffle=True)\n",
        "test_loader = data.DataLoader(test_dataset, batch_size=bs, num_workers=NUM_WORKERS, shuffle=False, drop_last=False)\n",
        "\n",
        "model = RotationPrediction(train_loader, n_classes=10, device=device)\n",
        "linear_classifier = model.construct_classifier()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr_initial, weight_decay=wd)\n",
        "optimizer_linear = torch.optim.Adam(linear_classifier.parameters(), lr=lr_initial)\n",
        "\n",
        "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=maxepochs)\n",
        "scheduler_linear = lr_scheduler.CosineAnnealingLR(optimizer_linear, T_max=maxepochs)\n",
        "\n",
        "top1Train=[]\n",
        "top5Train=[]\n",
        "top1Test=[]\n",
        "top5Test=[]\n",
        "\n",
        "for epoch in range(maxepochs):\n",
        "  top1Tr, top5Tr = train(train_loader, model, linear_classifier, optimizer, optimizer_linear, epoch, device)\n",
        "  top1Train.append(top1Tr)\n",
        "  top5Train.append(top5Tr)\n",
        "  top1Te, top5Te = val_loss, val_acc = validate(test_loader, model, linear_classifier, device)\n",
        "  top1Test.append(top1Te)\n",
        "  top5Test.append(top5Te)\n",
        "  scheduler.step()\n",
        "  scheduler_linear.step()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you want to save your model, we can use torch.save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.makedirs('models/', exist_ok=True)\n",
        "filename = 'models/checkpoint_rotation.pth.tar'\n",
        "torch.save({\n",
        "                'epoch': epoch + 1,\n",
        "                'state_dict': model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'scheduler': scheduler.state_dict(),\n",
        "                'state_dict_linear': linear_classifier.state_dict(),\n",
        "                'optimizer_linear': optimizer_linear.state_dict(),\n",
        "                'schedular_linear': scheduler_linear.state_dict()\n",
        "            }, filename)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we will load the same model as before but already pre-trained for 100 epochs.\n",
        "\n",
        "It's important to set the 'model.eval()' since we want to evaluate it and not train it (no gradient is computed and parameters are not updated)\n",
        "\n",
        "We will use the accuracy function as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import gdown\n",
        "\n",
        "os.makedirs('models/', exist_ok=True)\n",
        "# Download the Test set\n",
        "file_url = 'https://drive.google.com/uc?id=1NjMEStID7Oghzy_VfUmhw058RI_NyAqm'\n",
        "ckpt_pth = 'models/checkpoint_rotation.pth.tar'\n",
        "gdown.download(file_url, ckpt_pth, quiet=True)\n",
        "\n",
        "# Load checkpoint file of already trained model\n",
        "ckpt = torch.load(ckpt_pth)\n",
        "\n",
        "# Load Model parameters and set it into eval mode\n",
        "test_loader = data.DataLoader(test_dataset, batch_size=bs, num_workers=NUM_WORKERS, shuffle=False, drop_last=False)\n",
        "model = RotationPrediction(test_loader, n_classes=10, device=device)\n",
        "model.load_state_dict(ckpt['state_dict'])\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Load linear classifier model\n",
        "linear_classifier = model.construct_classifier()\n",
        "linear_classifier.load_state_dict(ckpt['state_dict_linear'])\n",
        "linear_classifier.to(device)\n",
        "linear_classifier.eval()\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "correct1, correct5 = 0, 0\n",
        "with torch.no_grad():\n",
        "    for images, target in test_loader:\n",
        "        images = images.to(device)\n",
        "        target = target.to(device)\n",
        "        out, zs = model(images)\n",
        "\n",
        "        logits = linear_classifier(zs)\n",
        "        logits=logits.to(device)\n",
        "        acc1, acc5 = accuracy(logits, target, topk=(1, 5))\n",
        "\n",
        "        correct1 += acc1.item() * logits.shape[0]\n",
        "        correct5 += acc5.item() * logits.shape[0]\n",
        "total = len(test_loader.dataset)\n",
        "\n",
        "testacc1= correct1 / total\n",
        "testacc5= correct5 / total\n",
        "\n",
        "print('Test Set')\n",
        "print(f'Top 1 Accuracy: {testacc1}, Top 5 Accuracy: {testacc5}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
