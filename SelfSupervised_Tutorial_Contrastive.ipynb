{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0JdZ_NoJMUp1"
      },
      "source": [
        "## Contrastive pre-training\n",
        "\n",
        "Here, we will try the SimCLR method.\n",
        "\n",
        "[1] T. Chen et al. “A Simple Framework for Contrastive Learning of Visual Representations”. In: ICML. 2020.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "8LnJlAFB8jfv",
        "outputId": "b4fc8ba0-150c-4629-83dd-693a11f76405"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "torch.manual_seed(42) # Setting the seed\n",
        "\n",
        "## Torchvision\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import STL10\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "## Plot Options\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib_inline.backend_inline\n",
        "import seaborn as sns\n",
        "plt.set_cmap(\"cividis\")\n",
        "%matplotlib inline\n",
        "sns.set()\n",
        "\n",
        "# In this notebook, we use data loaders with heavier computational processing. It is recommended to use as many\n",
        "# workers as possible in a data loader, which corresponds to the number of CPU cores\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.determinstic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Ensure that you are using GPU and all CPU workers\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)\n",
        "print(\"Number of workers:\", NUM_WORKERS)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Augmentation for Contrastive Learning\n",
        "\n",
        "One of the key points of SimCLR is the choice of the augmentation strategy. It composes many different geometric and iconographic transformations.\n",
        "We can implement them very efficiently and easily using the Dataset object of Pytorch.\n",
        "\n",
        "Since in SimCLR authors use 2 views, we do the same here. Please note that we could use more positives\n",
        "\n",
        "The transformations used are: (figure credit - [Ting Chen and Geoffrey Hinton](https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html)):\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/Lightning-AI/lightning-tutorials/raw/main/course_UvA-DL/13-contrastive-learning/simclr_data_augmentations.jpg\" width=\"800px\" style=\"padding-top: 10px; padding-bottom: 10px\"></center>\n",
        "\n",
        "When using ImageNet-derived datasets, the two most important transformations are: crop-and-resize, and color distortion.\n",
        "Interestingly, they need to be used together since, when combining randomly cropping and resizing, we might have two situations: (a) cropped image A provides a local view of cropped image B, or (b) cropped images C and D show neighboring views of the same image (figure credit - [Ting Chen and Geoffrey Hinton](https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html)).\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/Lightning-AI/lightning-tutorials/raw/main/course_UvA-DL/13-contrastive-learning/crop_views.svg\" width=\"400px\" style=\"padding-top: 20px; padding-bottom: 0px\"></center>\n",
        "\n",
        "While situation (a) requires the model to learn some sort of scale invariance to make crops A and B similar in the representation space, situation (b) is more challenging since the model needs to recognize an object beyond its limited view.\n",
        "However, the network can use the color information (color histograms) to create a useless link between the two patches, without learning generalizable high-level representations. For instance, it could focus on the color of the fur of the dog and on the color of the background to understand that the two patches belong to the same image. That's why, we need to compose crop-and-resize and color distortion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iE89L8tB9xAF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "class ContrastiveTransformations:\n",
        "    def __init__(self, img_size, s=1):\n",
        "      # transformations applied in SimCLR article\n",
        "      color_jitter = transforms.ColorJitter(0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s)\n",
        "      self.data_transform = transforms.Compose(\n",
        "          [\n",
        "              transforms.RandomResizedCrop(size=img_size),\n",
        "              transforms.RandomHorizontalFlip(),\n",
        "              transforms.RandomApply([color_jitter], p=0.8),\n",
        "              transforms.RandomGrayscale(p=0.2),\n",
        "              transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0)),\n",
        "              transforms.ToTensor(),\n",
        "              # imagenet stats\n",
        "              transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "          ]\n",
        "      )\n",
        "\n",
        "    def __call__(self, x):\n",
        "      # it outputs a tuple, namely 2 views (augmentations) fo the same image\n",
        "      return  self.data_transform(x), self.data_transform(x)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ABAd7xO_qA41"
      },
      "source": [
        "We create unlabeled, training and test Datasets.\n",
        "Please be careful since we have datasets where each instance has two views and datasets (e.g., test) where one instance has only one view. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXhGEmnT9voP",
        "outputId": "44d2ba94-329c-4053-dd87-7022fa3c9431"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import STL10\n",
        "\n",
        "img_transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "unlabeled_dataset_2viewsCon = STL10(root=\"./data\", split=\"unlabeled\", download=True, transform=ContrastiveTransformations(96))\n",
        "train_dataset = STL10(root=\"./data\", split=\"train\", download=True, transform=img_transforms)\n",
        "# just to show the effect of the augmentations and the classes\n",
        "train_dataset_2viewsCon = STL10(root=\"./data\", split=\"train\", download=True, transform=ContrastiveTransformations(96))\n",
        "test_dataset = STL10(root=\"./data\", split=\"test\", download=True, transform=img_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMXiU1qX2dqu",
        "outputId": "bcdfaba7-8f59-4d36-acb8-67dddd4c7ecb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# To check the classes in STL10\n",
        "classes=unlabeled_dataset_2viewsCon.classes\n",
        "print(classes)\n",
        "print('Number images in Unlabeled dataset:' ,len(unlabeled_dataset_2viewsCon))\n",
        "print(unlabeled_dataset_2viewsCon[0][0][0].shape) # this is one image (the first of the 2-views tuple)\n",
        "\n",
        "# Train dataset\n",
        "labels=train_dataset.labels # retrieve label of each sample\n",
        "print('Number images in Train dataset:' , len(train_dataset)) # retrieve length of dataset\n",
        "print(train_dataset[3][0].shape) # this is one image\n",
        "\n",
        "#Test dataset\n",
        "print('Number images in Test dataset:' ,len(test_dataset))\n",
        "print(test_dataset[0][0].shape) # this is one image"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Kioo1xFkQVRa"
      },
      "source": [
        "The Unlabeled dataset contains 100k images. Here, to limit memory requirement, we will use 10% of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbZTvz0RI6j_"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "sizeUnlabelled=5000\n",
        "\n",
        "unlabeled_dataset_2viewsCon_red,rest = random_split(unlabeled_dataset_2viewsCon, [sizeUnlabelled, len(unlabeled_dataset_2viewsCon)-sizeUnlabelled])\n",
        "len(unlabeled_dataset_2viewsCon_red)\n",
        "del unlabeled_dataset_2viewsCon # free memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIH9otn52ZR1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "def imshowSTL102views(datasetOrig,datasetTransform,rows=5,figsize=(8, 15)):\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    for i in range(1, 3*(rows-1)+2,3):\n",
        "      imgOrig = datasetOrig[i][0]\n",
        "      img1=datasetTransform[i][0][0]\n",
        "      img2=datasetTransform[i][0][1]\n",
        "\n",
        "      #REMOVE NORMALIZATION\n",
        "      mean = torch.tensor([0.485, 0.456, 0.406])\n",
        "      std = torch.tensor([0.229, 0.224, 0.225])\n",
        "      unnormalize = transforms.Normalize((-mean / std).tolist(), (1.0 / std).tolist())\n",
        "      # Clip values to range [0,1] -> possible rounding errors during normalization\n",
        "      imgOrig = np.clip(unnormalize(imgOrig).numpy(),0,1)\n",
        "      img1 = np.clip(unnormalize(img1).numpy(),0,1)\n",
        "      img2 = np.clip(unnormalize(img2).numpy(),0,1)\n",
        "\n",
        "      label = datasetOrig[i][1]\n",
        "      fig.add_subplot(rows, 3, i)\n",
        "      plt.title(datasetOrig.classes[label]+ ' , original')\n",
        "      plt.imshow(np.transpose(imgOrig, (1, 2, 0)))\n",
        "      plt.axis(\"off\")\n",
        "      fig.add_subplot(rows, 3, i+1)\n",
        "      plt.title(datasetOrig.classes[label] + ' , 1st view')\n",
        "      plt.imshow(np.transpose(img1, (1, 2, 0)))\n",
        "      plt.axis(\"off\")\n",
        "      fig.add_subplot(rows, 3, i+2)\n",
        "      plt.title(datasetOrig.classes[label] + ' , 2nd view')\n",
        "      plt.imshow(np.transpose(img2, (1, 2, 0)))\n",
        "      plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8KmQg476Jqtl",
        "outputId": "3eac4524-4c53-4b23-e2c6-3323368a5e7c"
      },
      "outputs": [],
      "source": [
        "imshowSTL102views(train_dataset,train_dataset_2viewsCon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGh01C9oQvuq"
      },
      "outputs": [],
      "source": [
        "del train_dataset_2viewsCon # To free memory"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, it's the most important part of the code.\n",
        "\n",
        "I remind you that the Siamese architecture of SimCLR is: (figure credit - [Ting Chen et al. ](https://arxiv.org/abs/2006.10029)):\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/Lightning-AI/lightning-tutorials/raw/main/course_UvA-DL/13-contrastive-learning/simclr_network_setup.svg\" width=\"350px\"></center>\n",
        "\n",
        "The employed loss is the InfoNCE loss:\n",
        "$$\n",
        "\\ell_{i,j}=-\\log \\frac{\\exp(\\text{sim}(z_i,z_j)/\\tau)}{\\sum_{k=1}^{2N}\\mathbb{1}_{[k\\neq i]}\\exp(\\text{sim}(z_i,z_k)/\\tau)}=-\\text{sim}(z_i,z_j)/\\tau+\\log\\left[\\sum_{k=1}^{2N}\\mathbb{1}_{[k\\neq i]}\\exp(\\text{sim}(z_i,z_k)/\\tau)\\right]\n",
        "$$\n",
        "where $\\tau$ is the temperature and the similarity measure is the cosine similarity:\n",
        "$$\n",
        "\\text{sim}(z_i,z_j) = \\frac{z_i^\\top \\cdot z_j}{||z_i||\\cdot||z_j||}\n",
        "$$\n",
        "The maximum cosine similarity possible is $1$, while the minimum is $-1$.\n",
        "\n",
        "After training, we will remove the projection head $g(\\cdot)$, and use $f(\\cdot)$ as a pretrained feature extractor.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKD0fMKqrOv7"
      },
      "outputs": [],
      "source": [
        "\n",
        "import shutil\n",
        "import yaml\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from tqdm import tqdm\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "class SimCLR():\n",
        "\n",
        "    def __init__(self, model, optimizer, scheduler, device, batch_size, temperature, epochs):\n",
        "        self.device=device\n",
        "        self.model = model.to(self.device)\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "        self.criterion = torch.nn.CrossEntropyLoss().to(self.device)\n",
        "        self.temperature=temperature\n",
        "        self.batch_size=batch_size\n",
        "        self.epochs=epochs\n",
        "\n",
        "\n",
        "    def info_nce_loss(self, features):\n",
        "\n",
        "        # This implementation is interesting. Do you understand why we create the array labels ?\n",
        "        labels = torch.cat([torch.arange(self.batch_size) for i in range(2)], dim=0)\n",
        "        labels = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()\n",
        "        labels = labels.to(self.device)\n",
        "\n",
        "        features = F.normalize(features, dim=1) # Normalize so that every sample is on the hyper-sphere\n",
        "\n",
        "        # compute cosine similarity between all couples of images\n",
        "        similarity_matrix = torch.matmul(features, features.T)\n",
        "\n",
        "        # discard the main diagonal from both: labels and similarities matrix\n",
        "        mask = torch.eye(labels.shape[0], dtype=torch.bool).to(self.device)\n",
        "        labels = labels[~mask].view(labels.shape[0], -1)\n",
        "        similarity_matrix = similarity_matrix[~mask].view(similarity_matrix.shape[0], -1)\n",
        "\n",
        "        # select and combine multiple positives\n",
        "        positives = similarity_matrix[labels.bool()].view(labels.shape[0], -1)\n",
        "\n",
        "        # select only the negatives the negatives\n",
        "        negatives = similarity_matrix[~labels.bool()].view(similarity_matrix.shape[0], -1)\n",
        "\n",
        "        logits = torch.cat([positives, negatives], dim=1)\n",
        "        labels = torch.zeros(logits.shape[0], dtype=torch.long).to(self.device)\n",
        "\n",
        "        logits = logits / self.temperature\n",
        "        return logits, labels\n",
        "\n",
        "    def train(self, train_loader):\n",
        "\n",
        "        scaler = GradScaler()\n",
        "\n",
        "        n_iter = 0\n",
        "        print(\"Start SimCLR training for {} epochs.\".format(self.epochs))\n",
        "\n",
        "        for epoch_counter in range(self.epochs):\n",
        "            for images, _ in tqdm(train_loader):\n",
        "                images = torch.cat(images, dim=0)\n",
        "\n",
        "                images = images.to(self.device)\n",
        "\n",
        "                with autocast():\n",
        "                    features = self.model(images)\n",
        "                    logits, labels = self.info_nce_loss(features)\n",
        "                    loss = self.criterion(logits, labels)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                scaler.scale(loss).backward()\n",
        "\n",
        "                scaler.step(self.optimizer)\n",
        "                scaler.update()\n",
        "\n",
        "                n_iter += 1\n",
        "\n",
        "            # warmup for the first 10 epochs\n",
        "            if epoch_counter >= 5:\n",
        "                self.scheduler.step()\n",
        "\n",
        "            print('Epoch: {}, Average loss: {:.4f}, lr: {:.4f}'.format(epoch_counter, loss / len(train_loader.dataset), self.scheduler.get_last_lr()[0] ))\n",
        "\n",
        "        print(\"Training has finished.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As before, we use a DataLoader.\n",
        "\n",
        "DataLoader wraps an iterable around the Dataset to enable easy access to the samples. The Dataset retrieves our dataset features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python multiprocessing to speed up data retrieval. DataLoader is an iterable that abstracts this complexity for us in an easy API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIfB8usLnw_H"
      },
      "outputs": [],
      "source": [
        "import torch.utils.data as data\n",
        "\n",
        "bs = 512\n",
        "\n",
        "train_unlabelled_loader = data.DataLoader(dataset=unlabeled_dataset_2viewsCon_red, batch_size=bs, shuffle=True, num_workers=NUM_WORKERS,pin_memory=True, drop_last=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we define the hyper-parameters, optimization, scheduler and launch the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "9iuDyoz2vmbL",
        "outputId": "472a7bec-a55a-4284-a071-8c7491c1845f"
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "max_epochs=100\n",
        "lr=0.003\n",
        "wd=1e-4\n",
        "outdim=128\n",
        "log=100\n",
        "temperature=0.07\n",
        "# Ensure that you are using GPU and all CPU workers\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "#model = ResNetSimCLR(base_model=\"resnet18\", out_dim=128)\n",
        "model = models.resnet18(weights=None, num_classes=128)\n",
        "dim_mlp = model.fc.in_features\n",
        "model.fc = nn.Sequential(nn.Linear(dim_mlp, dim_mlp), nn.ReLU(), model.fc)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_unlabelled_loader), eta_min=0, last_epoch=-1)\n",
        "\n",
        "simclr = SimCLR(model=model, optimizer=optimizer, scheduler=scheduler, device=device, batch_size=bs, temperature=temperature, epochs=max_epochs)\n",
        "simclr.train(train_unlabelled_loader)\n",
        "\n",
        "# save model checkpoints\n",
        "os.makedirs('models/', exist_ok=True)\n",
        "filename = 'models/resnet18_simclr_100epochs_stl10.pth.tar'\n",
        "torch.save({\n",
        "                'epoch': max_epochs,\n",
        "                'state_dict': simclr.model.state_dict()\n",
        "            }, filename)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "AS before, I have already trained the model and you cna retrieve it here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FER2FZWlIKJd",
        "outputId": "a68c32bd-99b2-4fb8-a154-2c5cf139c288"
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models\n",
        "import gdown\n",
        "\n",
        "# Creating dataset folder\n",
        "os.makedirs('models/', exist_ok=True)\n",
        "filename = 'models/resnet18_simclr_100epochs_stl10.pth.tar'\n",
        "\n",
        "# Download the pre-trained model\n",
        "file_url = 'https://drive.google.com/uc?id=11y_Kxt4pSBuYCx_rc-HDj-X0HshoZW_N'\n",
        "gdown.download(file_url, filename, quiet=True)\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "modelDownload = models.resnet18(weights=None, num_classes=10).to(device)\n",
        "checkpoint = torch.load(filename, map_location=device)\n",
        "state_dict= checkpoint['state_dict']\n",
        "modelDownload.load_state_dict(state_dict, strict=False)\n",
        "epoch = checkpoint['epoch']\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "AS previously explained, we can now use $f()$ to encode the samples discarding $g()$. Can you see where do we do it?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGMQg40HWGI8"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "import torch.nn as nn\n",
        "\n",
        "@torch.no_grad()\n",
        "def prepare_data_features(model, dataset, batchsize,  device):\n",
        "    # Prepare model\n",
        "    network = deepcopy(model)\n",
        "    network.fc = nn.Identity()  \n",
        "    network.eval()\n",
        "    network.to(device)\n",
        "\n",
        "    # Encode all images\n",
        "    data_loader = data.DataLoader(dataset, batch_size=batchsize, num_workers=NUM_WORKERS, shuffle=False, drop_last=False)\n",
        "\n",
        "    feats, labels = [], []\n",
        "    for batch_imgs, batch_labels in tqdm(data_loader):\n",
        "        batch_imgs = batch_imgs.to(device)\n",
        "        batch_feats = network(batch_imgs)\n",
        "        feats.append(batch_feats.detach().cpu())\n",
        "        labels.append(batch_labels)\n",
        "\n",
        "    feats = torch.cat(feats, dim=0)\n",
        "    labels = torch.cat(labels, dim=0)\n",
        "\n",
        "    # Sort images by labels\n",
        "    labels, idxs = labels.sort()\n",
        "    feats = feats[idxs]\n",
        "\n",
        "    return data.TensorDataset(feats, labels), [feats.numpy() , labels.numpy()]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use either the trained model or the donwladed model and encode the train and test images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdYnp20mVBPu",
        "outputId": "ae24c4ed-6339-41a5-cf04-245c47100a0e"
      },
      "outputs": [],
      "source": [
        "#modelTrained=simclr.model\n",
        "modelTrained=modelDownload\n",
        "trainloader, [train_feats, train_labels] = prepare_data_features(modelTrained, train_dataset, batchsize=256, device=device)\n",
        "testloader, [test_feats, test_labels] = prepare_data_features(modelTrained, test_dataset, batchsize=256, device=device)\n",
        "print(train_feats.shape, train_labels.shape)\n",
        "print(test_feats.shape, test_labels.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Instead than using the accuracy as before, now we train a logistic regression on the train dataset and evaluate it on the test dataset. This is called Linear Probe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5TR-EFodecU",
        "outputId": "d6bd14b5-0287-4127-b730-7b1694872453"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# compute linear probe results\n",
        "log_reg = LogisticRegression(max_iter=5000,random_state=0).fit(train_feats, train_labels)\n",
        "print(\"Linear probe on training : \", log_reg.score(train_feats, train_labels), \"Linear probe on test :\",  log_reg.score(test_feats, test_labels))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What if we simply used it a pre-trained model on ImageNet ? ... "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import torchvision.models as models\n",
        "\n",
        "modelImageNet = models.resnet18(weights='IMAGENET1K_V1').to(device)\n",
        "trainloader, [train_feats, train_labels] = prepare_data_features(modelImageNet, train_dataset, batchsize=256, device=device)\n",
        "testloader, [test_feats, test_labels] = prepare_data_features(modelImageNet, test_dataset, batchsize=256, device=device)\n",
        "# compute linear probe results\n",
        "log_reg = LogisticRegression(max_iter=5000,random_state=0).fit(train_feats, train_labels)\n",
        "print(\"Linear probe model pre-trained on ImageNet 1K on training : \", log_reg.score(train_feats, train_labels), \"Linear probe model pre-trained on ImageNet 1K on test :\",  log_reg.score(test_feats, test_labels))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It seems to work pretty well, why in your opinion ?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To go further, you could use the PatchCamelyon dataset (https://www.kaggle.com/datasets/andrewmvd/metastatic-tissue-classification-patchcamelyon).\n",
        "\n",
        "The PatchCamelyon benchmark is a image classification dataset. It consists of 327.680 color images (96 x 96px) extracted from histopathologic scans of lymph node sections. Each image is annoted with a binary label indicating presence of metastatic tissue. \n",
        "\n",
        "You can donwload it from pytorch vision or, if you are using Google Colab, directly from our Google drive. The pytorch version needs to be unzipped and there is not enough RAM memory..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gdown\n",
        "import zipfile\n",
        "\n",
        "# Creating dataset folder\n",
        "!mkdir /content/pcam\n",
        "\n",
        "# Download the Train set\n",
        "file_url = 'https://drive.google.com/uc?id=1ipIG12YWag54v2_2JIyfPiZDN0Eu3IjB'\n",
        "output_path = '/content/pcam/camelyonpatch_level_2_split_train_x.h5'\n",
        "gdown.download(file_url, output_path, quiet=False)\n",
        "\n",
        "# Download Valid set\n",
        "file_url = 'https://drive.google.com/uc?id=1emdhTV8J8Pv-SjKSoMzE_SbT04Ik2yUm'\n",
        "output_path = '/content/pcam/camelyonpatch_level_2_split_valid_x.h5'\n",
        "gdown.download(file_url, output_path, quiet=False)\n",
        "\n",
        "# Download Test set\n",
        "file_url = 'https://drive.google.com/uc?id=1dkeFapKSKm-wUtf9zicxiSHWIS0uxv8Z'\n",
        "output_path = '/content/pcam/camelyonpatch_level_2_split_test_x.h5'\n",
        "gdown.download(file_url, output_path, quiet=False)\n",
        "\n",
        "# Download the labels\n",
        "file_url = 'https://drive.google.com/uc?id=10ftBj2ZiiDESTsANdF-v8oh4NieinYPP'\n",
        "output_path = '/content/label.zip'\n",
        "gdown.download(file_url, output_path, quiet=False)\n",
        "\n",
        "# Unzip and move to data directory\n",
        "with zipfile.ZipFile(output_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(output_path[:-4])\n",
        "!mv /content/label/Labels/camelyonpatch_level_2_split_train_y.h5 /content/pcam\n",
        "!mv /content/label/Labels/camelyonpatch_level_2_split_test_y.h5 /content/pcam\n",
        "!mv /content/label/Labels/camelyonpatch_level_2_split_valid_y.h5 /content/pcam\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 128\n",
        "# Loading data\n",
        "transform = ToTensor()\n",
        "train_dataset = PCAM(root=\"\", download=False, split='train', transform=transform)\n",
        "val_dataset = PCAM(root=\"\", download=False, split='val', transform=transform)\n",
        "test_dataset = PCAM(root=\"\", download=False, split='test', transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=2, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "\n",
        "# Defining model and training options\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n",
        "\n",
        "\n",
        "### PCAM Dataset\n",
        "model = ViT((3, 96, 96), n_patches=12, n_blocks=2, hidden_d=8, n_heads=2, out_d=10).to(device)\n",
        "\n",
        "# model = ViT((1, 28, 28), n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10).to(device)\n",
        "# model = ViT((1, 28, 28), n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10).to(device)\n",
        "\n",
        "N_EPOCHS = 5\n",
        "LR = 0.005\n",
        "\n",
        "# Training loop\n",
        "optimizer = Adam(model.parameters(), lr=LR)\n",
        "criterion = CrossEntropyLoss()\n",
        "for epoch in trange(N_EPOCHS, desc=\"Training\"):\n",
        "    train_loss = 0.0\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1} in training\", position=0,leave=False):\n",
        "        x, y = batch\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        y_hat = model(x)\n",
        "        loss = criterion(y_hat, y)\n",
        "\n",
        "        train_loss += loss.detach().cpu().item() / len(train_loader)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{N_EPOCHS} loss: {train_loss:.2f}\")\n",
        "\n",
        "# Test loop\n",
        "with torch.no_grad():\n",
        "    correct, total = 0, 0\n",
        "    test_loss = 0.0\n",
        "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
        "        x, y = batch\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        y_hat = model(x)\n",
        "        loss = criterion(y_hat, y)\n",
        "        test_loss += loss.detach().cpu().item() / len(test_loader)\n",
        "\n",
        "        correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n",
        "        total += len(x)\n",
        "    print(f\"Test loss: {test_loss:.2f}\")\n",
        "    print(f\"Test accuracy: {correct / total * 100:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
